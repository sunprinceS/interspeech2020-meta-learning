<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Jui-Yang Hsu">

  
  
  
    
  
  <meta name="description" content="Interspeech 2020 Special Session: Meta Learning for Human Language Technology">

  
  <link rel="alternate" hreflang="en-us" href="https://sunprinces.github.io/interspeech2020-meta-learning/">

  


  
  
  
  <meta name="theme-color" content="hsl(339, 90%, 68%)">
  

  
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/interspeech2020-meta-learning/css/academic.css">

  




  


  
  <link rel="alternate" href="/interspeech2020-meta-learning/index.xml" type="application/rss+xml" title="Interspeech 2020 Special Session">
  

  <link rel="manifest" href="/interspeech2020-meta-learning/index.webmanifest">
  <link rel="icon" type="image/png" href="/interspeech2020-meta-learning/images/icon_hu193fc9a26a32044892cac699d9253167_196637_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/interspeech2020-meta-learning/images/icon_hu193fc9a26a32044892cac699d9253167_196637_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://sunprinces.github.io/interspeech2020-meta-learning/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Interspeech 2020 Special Session">
  <meta property="og:url" content="https://sunprinces.github.io/interspeech2020-meta-learning/">
  <meta property="og:title" content="Interspeech 2020 Special Session">
  <meta property="og:description" content="Interspeech 2020 Special Session: Meta Learning for Human Language Technology"><meta property="og:image" content="https://sunprinces.github.io/interspeech2020-meta-learning/images/icon_hu193fc9a26a32044892cac699d9253167_196637_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://sunprinces.github.io/interspeech2020-meta-learning/images/icon_hu193fc9a26a32044892cac699d9253167_196637_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    <meta property="og:updated_time" content="2020-06-01T13:00:00&#43;00:00">
  

  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite","url": "https://sunprinces.github.io/interspeech2020-meta-learning/"
}
</script>


  
    <script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Project",
  "@id": "https://sunprinces.github.io/interspeech2020-meta-learning/",
  "name": "Interspeech 2020",
  
  
  
  
  
  "url": "https://sunprinces.github.io/interspeech2020-meta-learning/"
}
</script>

  


  


  





  <title>Interspeech 2020 Special Session</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/interspeech2020-meta-learning/">Interspeech 2020 Special Session</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/interspeech2020-meta-learning/">Interspeech 2020 Special Session</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/interspeech2020-meta-learning/#calls" data-target="#calls"><span>Call for Papers</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/interspeech2020-meta-learning/#reading" data-target="#reading"><span>Reading</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/interspeech2020-meta-learning/#people" data-target="#people"><span>Organizers</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/interspeech2020-meta-learning/#program" data-target="#program"><span>Program</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/interspeech2020-meta-learning/#accepted_papers" data-target="#accepted_papers"><span>Accepted Papers</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/interspeech2020-meta-learning/#contact" data-target="#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  











<span class="js-widget-page d-none"></span>




  







  
  
  

  

  

  

  
    
    
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  <section id="intro" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      <h1>Meta Learning for Human Language Technology</h1>
      
      <!--weight:  5-->
<p><strong>Special session at INTERSPEECH 2020, Shanghai, China</strong></p>
<h2 id="description">Description</h2>
<p>Deep learning based human language technology (HLT), such as automatic speech recognition, intent and slot recognition, or dialog management, has become the mainstream of research in recent years and significantly outperforms conventional methods. The technology also has widespread applications in the industry. Several most famous examples include Siri, Alexa, Google Assistant, and Cortana. However, deep learning models are notorious for being data and computation hungry. These downsides limit the application of such models from deployment to different languages, domains, or styles, since collecting in-genre data and model training from scratch are costly.</p>
<p>Meta learning, or Learning to Learn, is one way to mitigate the above problems. Meta learning learns better learning algorithms, including better parameter initialization, optimization strategy, network architecture, distance metrics, etc., from multiple learning tasks. Meta learning has been showed the potential to allow faster fine-tuning, converge to better performance than model pretraining, and even achieve few-shot learning in several areas, including computer vision and translation.</p>
<p>The goal of this special session is to bring together researchers and practitioners working on meta learning in different HLT fields to discuss the state-of-the-art and new approaches, and to share their innovation, insights, and challenges, and to shed light on future research directions. We will explore how to improve learning efficiency in data usage and in computation with meta learning for HLT tasks. We also aim to align academic efforts with industrial challenges, to bridge the gap between research and real-world product deployment.</p>
<!--<div class="alert alert-note">
  <div>
    &ndash;&gt;</p>
<!--Important information-->
<!--
  </div>
</div>
-->

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  <section id="calls" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-12 col-lg-4 section-heading">
      <h1>Call for Papers</h1>
      
    </div>
    <div class="col-12 col-lg-8">
      <!--weight: 15-->
<p>The special session of Meta Learning for Human Language Technology invites papers of a theoretical and experimental nature on human language technology tasks with meta learning methodologies and their applications. The special session is part of the main INTERSPEECH conference in Shanghai, China. Relevant meta learning topics include (but are not limited to):</p>
<ul>
<li>Network architecture search</li>
<li>Learning optimizer</li>
<li>Learning model initialization</li>
<li>Learning metrics or distance measurement</li>
<li>Learning training algorithm</li>
<li>Few shot learning</li>
</ul>
<p>Human language technology topics include (but are not limited to):</p>
<ul>
<li>Automatic speech recognition</li>
<li>Speaker adaptation</li>
<li>Speaker identification</li>
<li>Speech synthesis</li>
<li>Voice conversion</li>
<li>Noise robustness</li>
<li>Spoken language understanding</li>
<li>Intent or slot recognition</li>
<li>Dialog management</li>
</ul>
<div class="alert alert-note">
  <div>
    <strong>Important Dates</strong></p>
<ul>
<li>Submission portal opens: February 15, 2020</li>
<li>Paper Submission: March 30, 2020</li>
<li>Notification of Acceptance: June 19, 2020</li>
<li>Camera-ready Paper Due: TBD</li>
<li>Special Session Date: TBD</li>
</ul>
  </div>
</div>
<h4 id="submissions">Submissions</h4>
<p>This special session is part of the main INTERSPEECH conference. Thus it utilizes the same submission portal, and follows the same submission policy, paper format, and review process. More information can be found:</p>
<ul>
<li>Submission portal: TBD</li>
<li>Submission policy: TBD</li>
<li>Paper format: TBD</li>
<li>Author ethics: TBD</li>
</ul>
<p>Following the same policy as INTERSPEECH main conference, double-submissions is not allowed if the entire works have been published in other peer-reviewed conferences or transaction. However, we invite authors to submit their work to multiple sessions in addition to this session in the submission portal. Conference and session committee will determine session assignment after acceptance.</p>
<p>If you have more questions about submission, please feel free to contact us via 
<a href="mailto:is.2020.meta.learning@gmail.com">is.2020.meta.learning@gmail.com</a>
.</p>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  <section id="reading" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      <h1>Reading</h1>
      
      <p>Meta learning is one of the fastest growing research areas in the deep learning scope. However there is no standard definition for meta learning. Usually the main goal is to design models that can learn new tasks rapidly with few in domain training examples, by having models to pre-learn from many, relevant or not, training tasks in a way that the models are easy to be generalized to new tasks. For better understanding the scope of meta learning, we provide several online courses and papers describing the works falling into the area. These works are just for showcasing, and we definitely encourage people with research not covered here but sharing the same goal mentioned above to submit.</p>
<h3 id="online-courses">Online Courses</h3>
<ul>
<li>
<a href="http://cs330.stanford.edu/" target="_blank" rel="noopener">CS 330: Deep Multi-Task and Meta Learning</a>
</li>
<li>
<a href="https://youtu.be/wurPYalweeo" target="_blank" rel="noopener">Hung-Yi Lee's Lecture</a>
 (in Mandarin)</li>
</ul>
<h3 id="papers">Papers</h3>
<h4 id="meta-learning-technology">Meta Learning Technology</h4>
<ul>
<li>Learning to Initialize:
<ul>
<li>Chelsea Finn, Pieter Abbeel, and Sergey Levine, “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks”, ICML, 2017</li>
<li>Sebastian Flennerhag, Pablo G. Moreno, Neil D. Lawrence, Andreas Damianou, Transferring Knowledge across Learning Processes, ICLR, 2019</li>
</ul>
</li>
<li>Learning to optimize:
<ul>
<li>Sachin Ravi, Hugo Larochelle, Optimization as a model for few-shot learning, ICLR, 2017</li>
<li>Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, Nando de Freitas, Learning to learn by gradient descent by gradient descent, NIPS, 2016</li>
</ul>
</li>
<li>Learning to compare
<ul>
<li>Jake Snell, Kevin Swersky, Richard S. Zemel, Prototypical Networks for Few-shot Learning, NIPS, 2017</li>
<li>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra, Matching Networks for One Shot Learning, NIPS, 2016</li>
<li>Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, Timothy M. Hospedales, Learning to Compare: Relation Network for Few-Shot Learning, CVPR, 2018</li>
</ul>
</li>
<li>Learning the whole learning algorithm
<ul>
<li>Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap, Meta-Learning with Memory-Augmented Neural Networks, ICML, 2016</li>
<li>Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel, A Simple Neural Attentive Meta-Learner, ICLR, 2018</li>
</ul>
</li>
<li>Network architecture search:
<ul>
<li>RL based
<ul>
<li>Barret Zoph, Quoc V. Le, Neural Architecture Search with Reinforcement Learning, ICLR 2017</li>
<li>Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le,  Learning Transferable Architectures for Scalable Image Recognition, CVPR, 2018</li>
<li>Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, Jeff Dean, Efficient Neural Architecture Search via Parameter Sharing, ICML, 2018</li>
</ul>
</li>
<li>Evolution based
<ul>
<li>Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc Le, Alex Kurakin, Large-Scale Evolution of Image Classifiers, ICML 2017</li>
<li>Esteban Real, Alok Aggarwal, Yanping Huang, Quoc V Le, Regularized Evolution for Image Classifier Architecture Search, AAAI, 2019</li>
<li>Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray Kavukcuoglu, Hierarchical Representations for Efficient Architecture Search, ICLR, 2018</li>
</ul>
</li>
<li>Supernetwork based
<ul>
<li>Hanxiao Liu, Karen Simonyan, Yiming Yang, DARTS: Differentiable Architecture Search, ICLR, 2019</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="applications-to-human-language-technology">Applications to Human Language Technology:</h4>
<ul>
<li>ASR
<ul>
<li>Jui-Yang Hsu, Yuan-Jui Chen, Hung-yi Lee, Meta Learning for End-to-End Low-Resource Speech Recognition, ICASSP 2020</li>
<li>Ondřej Klejch, Joachim Fainberg, Peter Bell, Learning to adapt: a meta-learning approach for speaker adaptation, INTERSPEECH 2018</li>
<li>Ondřej Klejch, Joachim Fainberg, Peter Bell, Steve Renals, Speaker Adaptive Training using Model Agnostic Meta-Learning, ASRU 2019</li>
</ul>
</li>
<li>Voice cloning
<ul>
<li>Yutian Chen, Yannis Assael, Brendan Shillingford, David Budden, Scott Reed, Heiga Zen, Quan Wang, Luis C. Cobo, Andrew Trask, Ben Laurie, Caglar Gulcehre, Aäron van den Oord, Oriol Vinyals, Nando de Freitas, Sample Efficient Adaptive Text-to-Speech, ICLR, 2019</li>
<li>Joan Serrà, Santiago Pascual, Carlos Segura, Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion, NeurIPS 2019</li>
</ul>
</li>
<li>Speaker Recognition:
<ul>
<li>Prashant Anand, Ajeet Kumar Singh, Siddharth Srivastava, Brejesh Lall, Few Shot Speaker Recognition using Deep Neural Networks, arXiv, 2019</li>
</ul>
</li>
<li>Keyword spotting:
<ul>
<li>Yangbin Chen, Tom Ko, Lifeng Shang, Xiao Chen, Xin Jiang, Qing Li, An Investigation of Few-Shot Learning in Spoken Term Classification, arXiv, 2018</li>
<li>Hanna Mazzawi, Javier Gonzalvo, Aleks Kracun, Prashant Sridhar, Niranjan Subrahmanya, Ignacio Lopez Moreno, Hyun Jin Park, Patrick Violette, Improving Keyword Spotting and Language Identification via Neural Architecture Search at Scale, INTERSPEECH, 2019</li>
</ul>
</li>
<li>Sound event detection:
<ul>
<li>Kazuki Shimada, Yuichiro Koyama, Akira Inoue, Metric Learning with Background Noise Class for Few-shot Detection of Rare Sound Events, arXiv, 2019</li>
<li>Szu-Yu Chou, Kai-Hsiang Cheng, Jyh-Shing Roger Jang, Yi-Hsuan Yang, Learning to match transient sound events using attentional similarity for few-shot sound recognition, ICASSP 2019</li>
<li>Shilei Zhang, Yong Qin, Kewei Sun, Yonghua Lin, Few-Shot Audio Classification with Attentional Graph Neural Networks, INTERSPEECH 2019</li>
</ul>
</li>
<li>Translation:
<ul>
<li>Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, Victor O.K. Li, Meta-Learning for Low-Resource Neural Machine Translation, EMNLP, 2018</li>
</ul>
</li>
<li>Dialogue Generation:
<ul>
<li>Kun Qian, Zhou Yu, Domain Adaptive Dialog Generation via Meta Learning, ACL 2019</li>
<li>Andrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, Pascale Fung, Personalizing Dialogue Agents via Meta-Learning, ACL 2019</li>
<li>Fei Mi, Minlie Huang, Jiyong Zhang, Boi Faltings, Meta-Learning for Low-resource Natural Language Generation in Task-oriented Dialogue Systems, IJCAI 2019</li>
<li>Yiping Song, Zequn Liu, Wei Bi, Rui Yan, Ming Zhang, earning to Customize Language Model for Personalized Conversation Systems, arXiv, 2019</li>
<li>Jen-Tzung Chien, Wei Xiang Lieow, Meta Learning for Hyperparameter Optimization in Dialogue System, INTERSPEECH, 2019</li>
</ul>
</li>
<li>Relation Classification
<ul>
<li>Abiola Obamuyide, Andreas Vlachos, Model-Agnostic Meta-Learning for Relation Classification with Limited Supervision, ACL 2019</li>
<li>Zhi-Xiu Ye, Zhen-Hua Ling, Multi-Level Matching and Aggregation Network for Few-Shot Relation Classification, ACL 2019</li>
<li>Mingyang Chen, Wen Zhang, Wei Zhang, Qiang Chen, Huajun Chen, Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs, EMNLP 2019</li>
<li>Avishek Joey Bose, Ankit Jain, Piero Molino, William L. Hamilton, Meta-Graph: Few shot Link Prediction via Meta Learning, arXiv, 2019</li>
<li>Xin Lv, Yuxian Gu, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu,Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations, EMNLP 2019</li>
<li>Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, William Yang Wang, One-Shot Relational Learning for Knowledge Graphs, EMNLP 2018</li>
<li>Tianyu Gao, Xu Han, Zhiyuan Liu, Maosong Sun, Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation Classification, AAAI. 2019</li>
</ul>
</li>
<li>Learning word embedding:
<ul>
<li>Ziniu Hu, Ting Chen, Kai-Wei Chang, Yizhou Sun,  Few-Shot Representation Learning for Out-Of-Vocabulary Words, ACL 2019</li>
<li>Jingyuan Sun, Shaonan Wang, Chengqing Zong, Memory, Show the Way: Memory Based Few Shot Word Representation Learning, EMNLP 2018</li>
</ul>
</li>
<li>More NLP applications:
<ul>
<li>Daya Guo, Duyu Tang, Nan Duan, Ming Zhou, Jian Yin, Coupling Retrieval and Meta-Learning for Context-Dependent Semantic Parsing, ACL, 2019</li>
<li>Qianhui Wu, Zijia Lin, Guoxin Wang, Hui Chen, Börje F. Karlsson, Biqing Huang, Chin-Yew Lin, Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources, AAAI 2020</li>
<li>Zhenjie Zhao, Xiaojuan Ma, Text Emotion Distribution Learning from Small Sample: A Meta-Learning Approach, EMNLP 2019</li>
<li>Trapit Bansal, Rishikesh Jha, Andrew McCallum, Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks, arXiv, 2019</li>
<li>Jiawei Wu, Wenhan Xiong, William Yang Wang, Learning to Learn and Predict: A Meta-Learning Approach for Multi-Label Classification, EMNLP 2019</li>
<li>Shengli Sun, Qingfeng Sun, Kevin Zhou, Tengchao Lv, Hierarchical Attention Prototypical Networks for Few-Shot Text Classification, EMNLP 2019</li>
<li>Ruiying Geng, Binhua Li, Yongbin Li, Xiaodan Zhu, Ping Jian, Jian Sun, Induction Networks for Few-Shot Text Classification, EMNLP, 2019</li>
<li>Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Yu Cheng, Gerald Tesauro, Haoyu Wang, Bowen Zhou, Diverse Few-Shot Text Classification with Multiple Metrics, ACL 2018</li>
<li>Ming Tan, Yang Yu, Haoyu Wang, Dakuo Wang, Saloni Potdar, Shiyu Chang, Mo Yu, Out-of-Domain Detection for Low-Resource Text Classification Tasks, EMNLP 2019</li>
<li>Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen-tau Yih, Xiaodong He, Natural Language to Structured Query Generation via Meta-Learning, NAACL 2018</li>
<li>Zi-Yi Dou, Keyi Yu, Antonios Anastasopoulos, Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks, EMNLP 2019</li>
</ul>
</li>
<li>Multi-model
<ul>
<li>Ryan Eloff, Herman A. Engelbrecht, Herman Kamper, MULTIMODAL ONE-SHOT LEARNING OF SPEECH AND IMAGES, ICASSP 2019</li>
<li>Dídac Surís, Dave Epstein, Heng Ji, Shih-Fu Chang, Carl Vondrick, Learning to Learn Words from Narrated Video, arXiv, 2019</li>
</ul>
</li>
</ul>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  

  

  
  

  
  
  

  
  
  
  
  

  
  

  <section id="people" class="home-section wg-people   "  >
    <div class="container">
      







<div class="row justify-content-center people-widget">
  
  <div class="col-md-12 section-heading">
    <h1>Organizers</h1>
    
  </div>
  

  

  
  

  
  <div class="col-md-12">
    <h2>&nbsp;</h2>
  </div>
  

  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/interspeech2020-meta-learning/authors/hungyi/"><img class="avatar avatar-circle" src="/interspeech2020-meta-learning/authors/hungyi/avatar_hu9d3a0b8f97d0a602006cb409c9cd596b_58980_270x270_fill_q90_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      
      <h2>Hung-Yi Lee</h2>
      <h3>National Taiwan University</h3>
      <h3>Associate Professor</h3>
      
      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/interspeech2020-meta-learning/authors/ngoc/"><img class="avatar avatar-circle" src="/interspeech2020-meta-learning/authors/ngoc/avatar_hu84fe7437002909d292d19be6c68dcb98_55098_270x270_fill_q90_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      
      <h2>Ngoc Thang Vu</h2>
      <h3>University of Stuttgart</h3>
      <h3>Professor</h3>
      
      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/interspeech2020-meta-learning/authors/shangwen/"><img class="avatar avatar-circle" src="/interspeech2020-meta-learning/authors/shangwen/avatar_hube98b48111b6c1a4c81f443802127d8f_68512_270x270_fill_q90_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      
      <h2>Shang-Wen Li</h2>
      <h3>Amzaon AWS AI</h3>
      <h3>Senior Applied Scientist</h3>
      
      
    </div>
  </div>
  
  
  
  
  
    
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <a href="/interspeech2020-meta-learning/authors/yu/"><img class="avatar avatar-circle" src="/interspeech2020-meta-learning/authors/yu/avatar_hu8314cdd3c0c87f4902511c1e311fafc1_69482_270x270_fill_q90_lanczos_center.jpg" alt="Avatar"></a>
    

    <div class="portrait-title">
      
      <h2>Yu Zhang</h2>
      <h3>Google Brain</h3>
      <h3>Research Scientist</h3>
      
      
    </div>
  </div>
  
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  <section id="program" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      <h1>Program</h1>
      
      <p>TBD</p>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  <section id="accepted_papers" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      <h1>Accepted Papers</h1>
      
      <p>TBD</p>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  

  

  
  

  
  
  

  
  
  
  
  

  
  

  <section id="contact" class="home-section wg-contact   "  >
    <div class="container">
      





<div class="row contact-widget">
  <div class="col-12 col-lg-4 section-heading">
    <h1>Contact</h1>
    
  </div>
  <div class="col-12 col-lg-8">
    

    

    If you have any questions or feedback, please feel free to contact us
    <ul class="fa-ul">

      
      <li>
        <i class="fa-li fas fa-envelope fa-2x" aria-hidden="true"></i>
        <span id="person-email"><a href="mailto:is.2020.meta.learning@gmail.com">is.2020.meta.learning@gmail.com</a></span>
      </li>
      

      

      
      

      

      

      

      
      

    </ul>

    

  </div>
</div>

    </div>
  </section>



      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    

    
    

    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/interspeech2020-meta-learning/js/academic.min.e5c8525332f417fe3589df9a6b25b6c4.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    © Interspeech 2020 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
